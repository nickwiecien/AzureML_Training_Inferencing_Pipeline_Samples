{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdcf30eb",
   "metadata": {},
   "source": [
    "## Azure ML - Sample Batch Prediction Pipeline\n",
    "This notebook demonstrates creation and execution of an Azure ML pipeline designed to load data from a remote source, to make predictions against that data using a previously registered ML model, and finally save that data to an external sink where it can be consumed by other LOB apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a9f7c",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData, PipelineEndpoint\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5299ff57",
   "metadata": {},
   "source": [
    "### Connect to Azure ML Workspace, Provision Compute Resources, and get References to Datastores\n",
    "Connect to workspace using config associated config file. Get a reference to you pre-existing AML compute cluster or provision a new cluster to facilitate processing. Finally, get references to your default blob datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "cpu_cluster_name = 'cpucluster'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=1)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "#Get default datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c821c32",
   "metadata": {},
   "source": [
    "### Get Sample Toy Dataset and Save to Default (Blob) Datastore\n",
    "Load data into a pandas dataframe directly from Scikit-Learn and save as a CSV to predefined location inside the default Azure ML datastore.\n",
    "\n",
    "Note: These data can be written to any AML-linked datastore using the SDK commands shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bbc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source data - using iris dataset from scikit below\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "input_df = pd.DataFrame(data.data, columns = data.feature_names)\n",
    "\n",
    "os.makedirs('./tmp', exist_ok=True)\n",
    "input_df.to_csv('./tmp/iris_data.csv', index=False)\n",
    "\n",
    "default_ds.upload(src_dir='./tmp',\n",
    "                 target_path='iris_data_scoring',\n",
    "                 overwrite=True)\n",
    "\n",
    "shutil.rmtree('./tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd57b3e",
   "metadata": {},
   "source": [
    "### Create Run Configuration\n",
    "The `RunConfiguration` defines the environment used across all python steps. You can optionally add additional conda or pip packages to be added to your environment. [More details here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py). In this example we retrieve the `Environment` which was used for model training to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e16ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.environment = Environment.get(ws, 'sample_env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597fa5df",
   "metadata": {},
   "source": [
    "### Define Output Datasets\n",
    "Below we define the configuration for datasets that will be passed between steps in our pipeline. Note, in all cases we specify the datastore that should hold the datasets and whether they should be registered following step completion or not. This can optionally be disabled by removing the register_on_complete() call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebdc93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencing_dataset = OutputFileDatasetConfig(name='inferencing_dataset', destination=(default_ds, 'inferencing_dataset/{run-id}')).read_delimited_files().register_on_complete(name='Inferencing_data')\n",
    "scored_dataset = OutputFileDatasetConfig(name='scored_dataset', destination=(default_ds, 'scored_dataset/{run-id}')).read_delimited_files().register_on_complete(name='Scored_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb110e",
   "metadata": {},
   "source": [
    "### Define Pipeline Parameters\n",
    "`PipelineParameter` objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Below we specify a pipeline parameter object model_name which will be used to reference the locally trained model that was uploaded and registered within the Azure ML workspace. Multiple pipeline parameters can be created and used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38431d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PipelineParameter(name='model_name', default_value='Iris_Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ccbcb",
   "metadata": {},
   "source": [
    "### Define Pipeline Steps\n",
    "The pipeline below consists of steps to gather and register data from a remote source, a scoring step where the registered model is used to make predictions on loaded, and a data publish step where scored data can be exported to a remote data source. All of the `PythonScriptSteps` have a corresponding *.py file which is referenced in the step arguments. Also, any `PipelineParameters` defined above can be passed to and consumed within these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9976017",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_inferencing_data_step = PythonScriptStep(\n",
    "    name='Get Inferencing Data',\n",
    "    script_name='get_inferencing_data.py',\n",
    "    arguments=[\n",
    "        '--inferencing_dataset', inferencing_dataset\n",
    "    ],\n",
    "    outputs=[inferencing_dataset],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "score_inferencing_data_step = PythonScriptStep(\n",
    "    name='Score Inferencing Data',\n",
    "    script_name='score_inferencing_data.py',\n",
    "    arguments=[\n",
    "        '--model_name', model_name,\n",
    "        '--scored_dataset', scored_dataset\n",
    "    ],\n",
    "    inputs=[inferencing_dataset.as_input(name='inferencing_data')],\n",
    "    outputs=[scored_dataset],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "publish_scored_data_step = PythonScriptStep(\n",
    "    name='Publish Scored Data',\n",
    "    script_name='publish_scored_data.py',\n",
    "    inputs=[scored_dataset.as_input(name='scored_data')],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a6700b",
   "metadata": {},
   "source": [
    "### Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4350b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[get_inferencing_data_step, score_inferencing_data_step, publish_scored_data_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef560d2",
   "metadata": {},
   "source": [
    "### Optional: Trigger a Pipeline Execution from the Notebook\n",
    "You can create an Experiment (logical collection for runs) and submit a pipeline run directly from this notebook by running the commands below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, 'sample-inferencing-pipeline-run')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa7df0",
   "metadata": {},
   "source": [
    "### Create a Published PipelineEndpoint\n",
    "Once we have created our pipeline we will look to retrain our model periodically as new data becomes available. By publishing our pipeline to a `PipelineEndpoint` we can iterate on our pipeline definition but maintain a consistent REST API endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa84ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "\n",
    "def published_pipeline_to_pipeline_endpoint(\n",
    "    workspace,\n",
    "    published_pipeline,\n",
    "    pipeline_endpoint_name,\n",
    "    pipeline_endpoint_description=\"Endpoint to my pipeline\",\n",
    "):\n",
    "    try:\n",
    "        pipeline_endpoint = PipelineEndpoint.get(\n",
    "            workspace=workspace, name=pipeline_endpoint_name\n",
    "        )\n",
    "        print(\"using existing PipelineEndpoint...\")\n",
    "        pipeline_endpoint.add_default(published_pipeline)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        # create PipelineEndpoint if it doesn't exist\n",
    "        print(\"PipelineEndpoint does not exist, creating one for you...\")\n",
    "        pipeline_endpoint = PipelineEndpoint.publish(\n",
    "            workspace=workspace,\n",
    "            name=pipeline_endpoint_name,\n",
    "            pipeline=published_pipeline,\n",
    "            description=pipeline_endpoint_description\n",
    "        )\n",
    "\n",
    "\n",
    "pipeline_endpoint_name = 'Classification Model Inferencing Pipeline'\n",
    "pipeline_endpoint_description = 'Sample pipeline for retrieving and scoring data using a classification model based on the Iris Setosa dataset'\n",
    "\n",
    "published_pipeline = pipeline.publish(name=pipeline_endpoint_name,\n",
    "                                     description=pipeline_endpoint_description,\n",
    "                                     continue_on_step_failure=False)\n",
    "\n",
    "published_pipeline_to_pipeline_endpoint(\n",
    "    workspace=ws,\n",
    "    published_pipeline=published_pipeline,\n",
    "    pipeline_endpoint_name=pipeline_endpoint_name,\n",
    "    pipeline_endpoint_description=pipeline_endpoint_description\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
